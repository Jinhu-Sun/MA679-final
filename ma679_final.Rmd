---
title: "ma679_final"
author: "Jinhu Sun"
date: "2024-04-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction:

Hospital readmissions, defined as a patient's return to a healthcare facility within 30 days after discharge, represent a significant challenge in healthcare management, impacting both patient outcomes and healthcare costs. Understanding the factors that contribute to these readmissions is crucial for developing strategies to reduce their occurrence and improve the quality of care.

This project aims to investigate the relationship between hospital readmissions and key factors such as age, gender, and specific diseases and so on. Age and gender have been recognized in previous studies as influential variables that may affect the likelihood of readmission due to varying health conditions and physiological differences. By analyzing how these demographic factors influence readmission rates, this study seeks to identify patterns that could help healthcare providers tailor post-discharge plans more effectively.

Moreover, the study will delve into the impact of specific diseases classified under standardized codes begin with H70, 0B11 and 0CTS. By focusing on these conditions, we aim to understand the unique challenges associated with each and their contribution to readmission risk. Identifying the relationship between these diseases and readmission can inform specialized care strategies and lead to improved patient outcomes.

Through comprehensive data analysis, this project will yield insights into the factors influencing readmission rates, enabling healthcare providers to devise effective interventions and personalized care approaches to minimize unnecessary hospital readmissions. Ultimately, the goal is to enhance patient care, optimize resource utilization, and improve overall healthcare delivery.


## Data cleaning

The initial step in our project involves thorough data cleaning and preparation, utilizing the NRD_2018_Core dataset. This dataset contains essential information, including patient age, gender, ICD-10-CM and ICD-10-PCS diagnosis codes, NRD visit links, and more.

Our primary objective is to identify patients who have been hospitalized more than once. Only those who have returned to the hospital can potentially be considered for readmission. To ascertain readmission status, we employ the formula: NRD_DaysToEvents(up to date) - NRD_DaysToEvents(previous) - Length of Stay (LOS). Here, NRD_DaysToEvents represents the timing interval used to determine the days between discharges and subsequent admissions. If this calculated interval is 30 days or fewer, the readmission_mark variable is set to 1; otherwise, it is set to 0.

Following the determination of potential readmissions, we focus on extracting data pertaining to specific diseases of interest, identified by standardized codes. These include diseases beginning with "H70" in the ICD-10 DX1 to DX40 columns, and those beginning with "0B11" and "0CTS" in the ICD-10 PR1 to PR25 columns. However, since the dataset labels diseases using full codes such as H7011, we need to modify our approach. To accommodate this, we convert the dataset such that it includes a binary matrix. In this matrix, column names correspond to these standardized codes, and cell values are either 0 or 1â€”indicating the absence or presence of a particular disease in each patient, respectively. This transformation simplifies the analysis and allows us to directly correlate specific diseases with readmission rates.

```{r}
library(reticulate)
library(data.table)
library(dplyr)
library(tidyr)
```

```{r}
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_Hospital.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)
```

```{r}
hospital2018 <- fread('/restricted/projectnb/ma679/Data/NRD_2018_Hospital.CSV', col.names = output)
```


```{r}
# Core
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_Core.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)
core2018 <- fread('/restricted/projectnb/ma679/Data/NRD_2018_Core.CSV', nrows = 1000000, col.names = output)
```


```{r}
# Diagnosis and Procedure Groups File (DPGF)
 
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_DX_PR_GRPS.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)
 
dpgf <- fread('/restricted/projectnb/ma679/Data/NRD_2018_DX_PR_GRPS.CSV', nrows = 1000000, col.names = output)
```


```{r}
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_Severity.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)

severity2018 <- fread('/restricted/projectnb/ma679/Data/NRD_2018_Severity.CSV', nrows = 1000000, col.names = output)
```

```{r}
# check readmission
#processed_data <- core2018 %>%
#  arrange(NRD_VisitLink, NRD_DaysToEvent) %>%
#  group_by(NRD_VisitLink) %>%
#  mutate(
#    prev_days_to_event = lag(NRD_DaysToEvent),
#    prev_length_of_stay = lag(LOS),
#    Days_Between_Admissions = NRD_DaysToEvent - prev_days_to_event - prev_length_of_stay,
#    Days_Between_Admissions = if_else(is.na(Days_Between_Admissions), 0, Days_Between_Admissions)
#  )

####################################################
processed_data <- core2018 %>%
  arrange(NRD_VisitLink, NRD_DaysToEvent) %>%
  group_by(NRD_VisitLink) %>%
  mutate(
    prev_days_to_event = lag(NRD_DaysToEvent),
    prev_length_of_stay = lag(LOS),
    Days_Between_Admissions = NRD_DaysToEvent - prev_days_to_event - prev_length_of_stay
  ) %>%
  mutate(
    Days_Between_Admissions = lead(Days_Between_Admissions)  # Shift the days between admissions to the previous row
  )

```


```{r}
#duplicates <- processed_data %>%
 # group_by(NRD_VisitLink) %>%     
  #filter(n() > 1) %>%   
  #ungroup()  

#duplicates <- duplicates %>%
  #filter(Days_Between_Admissions <= 30)

############### mark non-duplicate ID as 0 and "days_between_admission" > 30 also 0 #################
duplicates <- processed_data %>%
  group_by(NRD_VisitLink) %>%
  mutate(
    admission_count = n(),  # Count the number of admissions per patient
    readmission_mark = ifelse(admission_count == 1 | Days_Between_Admissions > 30, 0, 1)
  ) %>%
  ungroup()

duplicates <- duplicates %>%
  mutate(
    readmission_mark = if_else(is.na(readmission_mark), 0, readmission_mark)
  )


filtered_data123 <- duplicates %>%
  filter(
    rowSums(sapply(select(., I10_PR1:I10_PR25), function(x) {
      grepl("^0B11", x) | x %in% c("0CTS0ZZ", "0CTS3ZZ", "0CTS4ZZ", "0CTS7ZZ", "0CTS8ZZ")
    })) > 0 |
    rowSums(sapply(select(., I10_DX1:I10_DX40), function(x) {
      grepl("^H70", x)
    })) > 0
  )
```

```{r}
selected_data <- select(filtered_data123, "NRD_VisitLink",I10_PR1:I10_PR25,I10_DX1:I10_DX40,AGE,readmission_mark,NRD_VisitLink,FEMALE,LOS)
```

```{r}
procedure_columns <- paste0("I10_PR", 1:25)

diagnosis_columns <- paste0("I10_DX", 1:40)

replace_conditions <- function(x) {
  if_else(
    grepl("^0B11", x) | 
    grepl("^H70", x) | 
    x %in% c("0CTS0ZZ", "0CTS3ZZ", "0CTS4ZZ", "0CTS7ZZ", "0CTS8ZZ"),
    x,
    "0" 
  )
}
```

```{r}
df <- selected_data %>%
  mutate(across(all_of(c(procedure_columns, diagnosis_columns)), replace_conditions))
```


```{r}
unique_values <- df %>%
  select(I10_DX1:I10_DX40) %>%
  lapply(unique)
combined_unique_values <- unlist(unique_values)
overall_unique_values <- unique(combined_unique_values)
unique_H70 <- overall_unique_values[2:length(overall_unique_values)]
unique_H70
```
```{r}
overall_unique_values
```


```{r}
H70_data <- df[ , 27:66]
H70_dataframe <- matrix(0, nrow = nrow(df), ncol = length(unique_H70))
colnames(H70_dataframe) <- unique_H70
for(i in 1:40){
  DX_vector <- H70_data[ , i]
  H70_dataframe_iter <- matrix(0, nrow = nrow(df), ncol = length(unique_H70))
  for(j in 1:nrow(DX_vector)){
      if(DX_vector[j,] %in% colnames(H70_dataframe)){
        H70_dataframe_iter[j, which(colnames(H70_dataframe) == as.character(DX_vector[j, ]))] <- 1
      }
  }
  H70_dataframe <- H70_dataframe + H70_dataframe_iter
}
```

```{r}
unique_PR <- df %>%
  select(I10_PR1:I10_PR25) %>%
  lapply(unique)
combined_unique_PR <- unlist(unique_PR)
overall_unique_PR <- unique(combined_unique_PR)
overall_unique_PR
unique_PR <- overall_unique_PR[2:length(overall_unique_PR)]
unique_PR
```

```{r}
PR_data <- df[ , 2:26]
PR_dataframe <- matrix(0, nrow = nrow(df), ncol = length(unique_PR))
colnames(PR_dataframe) <- unique_PR
for(i in 1:25){
  PR_vector <- PR_data[ , i]
  PR_dataframe_iter <- matrix(0, nrow = nrow(df), ncol = length(unique_PR))
  for(j in 1:nrow(PR_vector)){
      if(PR_vector[j,] %in% colnames(PR_dataframe)){
        PR_dataframe_iter[j, which(colnames(PR_dataframe) == as.character(PR_vector[j, ]))] <- 1
      }
  }
  PR_dataframe <- PR_dataframe + PR_dataframe_iter
}
```

```{r}
total <- cbind(df, H70_dataframe)
dx_cols <- paste0("I10_DX", 1:40)
pr_cols <- paste0("I10_PR", 1:25)
total <- total %>% select(-all_of(c(dx_cols, pr_cols)))
total <- cbind(total, PR_dataframe)

total <- total %>%
  rename(
    B110F4 = `0B110F4`,
    B113F4 = `0B113F4`,
    B110Z4 = `0B110Z4`,
    CTS0ZZ = `0CTS0ZZ`,
    B114F4 = `0B114F4`,
    B113Z4 = `0B113Z4`,
    B110D6 = `0B110D6`
  )
```

##########################Modeling starts here#############################################

## Logistic regression with elastic-net regularization

The focus is on understand the factors contributing to hospital readmissions. Demographic factors like age and gender, as well as specific diseases, are being used to identify patterns. As it is a classification problem, the first thing that comes in mind is the logistic regression.

First we did a data transformation, key transformations involved converting selected columns to factors to ensure correct data handling during modeling. Continuous variables like age and length of stay (LOS) were scaled to normalize their values. The use of factors and scaling aids in effectively interpreting these variables in the predictive modeling process. And then we split the data into 75% training data and 25% testing data.


```{r}
library(caret)
library(ROSE)
library(randomForest)
library(pROC)
library(lme4)
library(vcd)
library(Hmisc)
```

```{r}
total <- total %>%
  mutate(across(6:ncol(total), factor))

total$AGE <- scale(total$AGE)
total$LOS <- scale(total$LOS)
total$readmission_mark <- as.factor(total$readmission_mark)
total$FEMALE <- as.factor(total$FEMALE)
```

```{r}
set.seed(123)
trainIndex <- createDataPartition(total$readmission_mark, p = 0.75, list = FALSE)
trainData <- total[trainIndex,]
testData <- total[-trainIndex,]
```

```{r}
table(trainData$readmission_mark)
```

### Handling Imbalanced Data

Here we can see that our data is extremely imbalanced 2247 non-readmission and 257 readmission. As we did not realized in the first time, we ran an logistic regression directly on the imbalanced data. The result was surprisingly good, with an accuracy of 0.898. However, this result is misleading due to the fact that our model basically never predict readmission patient in the result and the testing data also contains much more non-readmission patient than readmission patients.

Synthetic Minority Over-sampling Technique (SMOTE) was applied to balance the dataset, with this methodology we can get equal representation of outcomes. This technique synthetically generates new instances of the minority class using existing instances. It was achieved by over sampling, the output is stored in "trainDataBalanced", which will now have an equal or nearly equal number of instances for both classes of the readmission_mark (readmitted and not readmitted), totaling up to 20,000 instances as specified.

```{r}
# Applying SMOTE for imbalanced data handling
trainDataBalanced <- ovun.sample(readmission_mark ~ AGE+FEMALE+LOS+H7091+H7011+H70002+H70001+H7012+H7092+H70009+H70091+H7093+H70003+H70891+H70092+H7090+H7013+H70221+H70892+H70201+H7010+H70093+H70209+H70899+H70012+H70893+H70212+H70222+B110F4+B113F4+B110Z4+CTS0ZZ+B114F4+B113Z4+B110D6, data = trainData, method = "over", N = 20000)$data
```

### Modeling

Like I said before, a logistic regression model with elastic-net regularization was chosen due to its capability to handle both l1 (lasso) and l2 (ridge) penalties, useful in feature selection and regularization to prevent over fitting. The model was trained using a cross-validation approach with 20 folds to ensure that the model generalizes well on unseen data. 

```{r}
train_control <- trainControl(
  method = "cv",    
  number = 20,           
  verboseIter = TRUE, 
  savePredictions = "final" 
)

```

```{r}
# Define the tuning grid correctly with 'alpha' and 'lambda'
tune_grid <- expand.grid(
  alpha = c(0.0001, 0.001, 0.01, 0.1, 1),  # Elastic-net mixing parameter
  lambda = c(1e-4, 1e-3, 1e-2, 1e-1, 1)   # Regularization parameter
)

```

```{r}
set.seed(123)
model <- train(
  readmission_mark ~ AGE + FEMALE + LOS + H7091 + H7011 + H70002 + H70001 + H7012 + H7092 + H70009 + H70091 + H7093 + H70003 + H70891 + H70092 + H7090 + H7013 + H70221 + H70892 + H70201 + H7010 + H70093 + H70209 + H70899 + H70012 + H70893 + H70222 + B110F4 + B113F4 + B110Z4 + B114F4 + B113Z4 + B110D6 + CTS0ZZ,
  data = trainDataBalanced,       
  method = "glmnet",    
  trControl = train_control,
  tuneGrid = tune_grid,   
  metric = "Accuracy"   
)
```

```{r}
# Print the best model's details
print(model)

predictions <- predict(model, newdata = testData)
confusionMatrix(predictions, testData$readmission)
```
### Model Evaluation

The model's performance was evaluated using a confusion matrix. The final model selected had an alpha of 1e-04 and lambda of 0.1 based on its performance metrics. Although the sensitivity was extremely low, indicating poor performance in correctly identifying positive cases (readmissions), the specificity was perfect, meaning all non-readmission cases were identified correctly.


## Predictive Modeling Using Neural Networks

Using a deep learning model created with "Keras", for predicting hospital readmissions offers several advantages that align well with the objectives. First, deep learning Handles Complexity. The complexity of healthcare data, which includes a variety of predictors ranging from demographic variables to detailed medical codes, requires a model capable of capturing intricate patterns and interactions among variables. Neural networks are well-suited for this task due to their ability to learn non-linear relationships and interactions between features. Also, The problem in our hand is a binary classification (readmitted/not readmitted within 30 days), and neural networks can be effectively tuned for such outcomes, particularly through the use of appropriate loss functions like binary_crossentropy and activation functions like the sigmoid in the output layer.


```{r}
#Test my python enviroment
#library(reticulate)
#reticulate::install_python(version = '<version>')
py_run_string("print('Hello from Python')")
```

```{r}
py_config()
#py_install("tensorflow")

```
```{r}
X <- select(total, -readmission_mark)
y <- total$readmission_mark
```

### Data Preparation

We use the recipe() function to defines the outcome variable (readmission_mark) and the predictor variables (like AGE, FEMALE, various disease codes). Then step_dummy function is used to automatically convert all nominal (categorical) variables into dummy/indicator variables, except for the outcome variable. This transformation is crucial for models that only accept numerical input. And finally, the data was split into 75% training and 25% testing data. And we also applied SMOTE for imbalanced data handling.

```{r}
library(recipes)
library(rsample)

recipe_obj <- recipe(readmission_mark ~ AGE+FEMALE+H7091+H7011+H70002+H70001+H7012+H7092+H70009+H70091+H7093+H70003+H70891+H70092+H7090+H7013+H70221+H70892+H70201+H7010+H70093+H70209+H70899+H70012+H70893+H70212+H70222+B110F4 + B113F4 + B110Z4 + B114F4 + B113Z4 + B110D6, data = total) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  prep(training = total, retain = TRUE)
```

```{r}
X_encoded <- bake(recipe_obj, new_data = NULL)
```

```{r}
set.seed(123)  # for reproducibility
split <- initial_split(X_encoded, prop = 0.75)
train_data <- training(split)
test_data <- testing(split)

#Applying SMOTE for imbalanced data handling
train_data <- ovun.sample(readmission_mark ~ ., data = train_data, method = "over", N = 20000)$data
```


### Model Architecture

First dense layer is the input layer of the model, consisting of 16 units (neurons) and using the ReLU (Rectified Linear Unit) activation function. According to my search ReLU is a popular choice for hidden layers because it introduces non-linearity to the model, allowing the model to learn more complex patterns in the data. 

Second dense layer is the output layer, with just one unit. Since our task is binary classification, a single unit is sufficient. The activation function for this layer is sigmoid for binary classification to output probabilities between 0 and 1.

The model compilation in our code involves configuring the neural network for training, specifying how it learns and how its performance should be measured. By compiling the model with these settings our neural network to minimize the binary crossentropy during training, adjust its weights using the Adam optimization algorithm, and evaluate its success based on the accuracy metric.

We set the number of times the model will work through the entire training dataset to 100 and batch_size (The number of samples per gradient update for training) to 32.

```{r}
library(keras3)
```

```{r}
train_features <- as.matrix(train_data %>% select(-readmission_mark))
train_target <- train_data$readmission_mark
```

```{r}
num_features <- ncol(train_data %>% select(-readmission_mark))
print(num_features)
```

```{r}
# Resetting the Keras session might help in some cases
# k_clear_session()

model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(num_features)) %>%
  layer_dense(units = 1, activation = 'sigmoid')  

model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

# Convert training data to matrix format
train_features <- as.matrix(train_data %>% select(-readmission_mark))
train_target <- train_data$readmission_mark

# Fit the model
history <- model %>% fit(
  x = train_features, 
  y = train_target,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.25
)
```
```{r}
# Convert history to a dataframe
history_df <- as.data.frame(history$metrics)

# Add epoch column
history_df$epoch <- seq_len(nrow(history_df))

# Gather into long format for ggplot2
long_history <- pivot_longer(history_df, cols = -epoch, names_to = "metric", values_to = "value")

# Plot using ggplot2
ggplot(long_history, aes(x = epoch, y = value, color = metric)) +
  geom_line() +
  facet_wrap(~metric, scales = 'free_y') +
  theme_minimal() +
  labs(title = "Training History", x = "Epoch", y = "Metric Value")

```


### Result for Modeling

The training accuracy is relatively stable, indicating that the model is consistently learning or has reached a stable state of understanding the training data. However, the validation accuracy remains constant, suggesting that the model is not improving its performance on unseen data as training progresses. This could be a sign of overfitting, where the model performs well on the training data but fails to generalize well to new, unseen data.

And the right side Graph shows the training loss and validation loss over the same number of epochs. The training loss decreases and then stabilizes, which is typical as a model learns from the training data. The validation loss, however, is quite noisy, indicating variability in how well the model's predictions match the actual outputs of the validation data across epochs.

```{r}
test_features <- as.matrix(test_data %>% select(-readmission_mark))
test_target <- test_data$readmission_mark

evaluation <- model %>% evaluate(
  x = test_features,
  y = test_target,
  verbose = 1
)

# Print out the loss and accuracy
print(paste("Loss:", evaluation$loss))
print(paste("Accuracy:", evaluation$acc))

# Making predictions
predictions <- model %>% predict(test_features)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

confusionMatrix(as.factor(predicted_classes), as.factor(test_target))

```
### Result for Testing

The accuracy is very low (around 16.89%), and the loss is relatively high. This indicates that the model is not fitting or generalizing well to the test data. Also, a high false negative rate is observed. The model is heavily biased towards predicting the majority class (0), as seen from the specificity and negative predictive value.


### Conclution

Since both of our are in the 10% to 15% accuracy range, it suggesting that the current model configuration, including the neural network architecture and the data handling (SMOTE), is inadequate for the task. It does not capture the complexities or the patterns in the dataset necessary to accurately predict readmissions. Also despite efforts to balance the dataset using SMOTE, the model's ability to predict the minority class is significantly lacking. This is a sign which suggesting the need for revisiting the approach to handling imbalanced data.