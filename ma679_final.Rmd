---
title: "ma679_final"
author: "Jinhu Sun"
date: "2024-04-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(reticulate)
library(data.table)
library(dplyr)
library(tidyr)
```

```{r}
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_Hospital.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)
```

```{r}
hospital2018 <- fread('/restricted/projectnb/ma679/Data/NRD_2018_Hospital.CSV', col.names = output)
```


```{r}
# Core
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_Core.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)
core2018 <- fread('/restricted/projectnb/ma679/Data/NRD_2018_Core.CSV', nrows = 1000000, col.names = output)
```


```{r}
# Diagnosis and Procedure Groups File (DPGF)
 
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_DX_PR_GRPS.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)
 
dpgf <- fread('/restricted/projectnb/ma679/Data/NRD_2018_DX_PR_GRPS.CSV', nrows = 1000000, col.names = output)
```


```{r}
command <- "awk 'NR > 19 {print substr($0, 41, 29)}' '/restricted/projectnb/ma679/Data/FileSpecifications/FileSpecifications_NRD_2018_Severity.TXT'"
output <- trimws(system(command, intern = TRUE))
print(output)

severity2018 <- fread('/restricted/projectnb/ma679/Data/NRD_2018_Severity.CSV', nrows = 1000000, col.names = output)
```

```{r}
# check readmission
#processed_data <- core2018 %>%
#  arrange(NRD_VisitLink, NRD_DaysToEvent) %>%
#  group_by(NRD_VisitLink) %>%
#  mutate(
#    prev_days_to_event = lag(NRD_DaysToEvent),
#    prev_length_of_stay = lag(LOS),
#    Days_Between_Admissions = NRD_DaysToEvent - prev_days_to_event - prev_length_of_stay,
#    Days_Between_Admissions = if_else(is.na(Days_Between_Admissions), 0, Days_Between_Admissions)
#  )

####################################################
processed_data <- core2018 %>%
  arrange(NRD_VisitLink, NRD_DaysToEvent) %>%
  group_by(NRD_VisitLink) %>%
  mutate(
    prev_days_to_event = lag(NRD_DaysToEvent),
    prev_length_of_stay = lag(LOS),
    Days_Between_Admissions = NRD_DaysToEvent - prev_days_to_event - prev_length_of_stay
  ) %>%
  mutate(
    Days_Between_Admissions = lead(Days_Between_Admissions)  # Shift the days between admissions to the previous row
  )

```


```{r}
#duplicates <- processed_data %>%
 # group_by(NRD_VisitLink) %>%     
  #filter(n() > 1) %>%   
  #ungroup()  

#duplicates <- duplicates %>%
  #filter(Days_Between_Admissions <= 30)

############### mark non-duplicate ID as 0 and "days_between_admission" > 30 also 0 #################
duplicates <- processed_data %>%
  group_by(NRD_VisitLink) %>%
  mutate(
    admission_count = n(),  # Count the number of admissions per patient
    readmission_mark = ifelse(admission_count == 1 | Days_Between_Admissions > 30, 0, 1)
  ) %>%
  ungroup()

duplicates <- duplicates %>%
  mutate(
    readmission_mark = if_else(is.na(readmission_mark), 0, readmission_mark)
  )


filtered_data123 <- duplicates %>%
  filter(
    rowSums(sapply(select(., I10_PR1:I10_PR25), function(x) {
      grepl("^0B11", x) | x %in% c("0CTS0ZZ", "0CTS3ZZ", "0CTS4ZZ", "0CTS7ZZ", "0CTS8ZZ")
    })) > 0 |
    rowSums(sapply(select(., I10_DX1:I10_DX40), function(x) {
      grepl("^H70", x)
    })) > 0
  )
```

```{r}
selected_data <- select(filtered_data123, "NRD_VisitLink",I10_PR1:I10_PR25,I10_DX1:I10_DX40,AGE,readmission_mark,NRD_VisitLink,FEMALE,LOS)
```

```{r}
procedure_columns <- paste0("I10_PR", 1:25)

diagnosis_columns <- paste0("I10_DX", 1:40)

replace_conditions <- function(x) {
  if_else(
    grepl("^0B11", x) | 
    grepl("^H70", x) | 
    x %in% c("0CTS0ZZ", "0CTS3ZZ", "0CTS4ZZ", "0CTS7ZZ", "0CTS8ZZ"),
    x,
    "0" 
  )
}
```

```{r}
df <- selected_data %>%
  mutate(across(all_of(c(procedure_columns, diagnosis_columns)), replace_conditions))
```


```{r}
unique_values <- df %>%
  select(I10_DX1:I10_DX40) %>%
  lapply(unique)
combined_unique_values <- unlist(unique_values)
overall_unique_values <- unique(combined_unique_values)
unique_H70 <- overall_unique_values[2:length(overall_unique_values)]
unique_H70
```
```{r}
overall_unique_values
```


```{r}
H70_data <- df[ , 27:66]
H70_dataframe <- matrix(0, nrow = nrow(df), ncol = length(unique_H70))
colnames(H70_dataframe) <- unique_H70
for(i in 1:40){
  DX_vector <- H70_data[ , i]
  H70_dataframe_iter <- matrix(0, nrow = nrow(df), ncol = length(unique_H70))
  for(j in 1:nrow(DX_vector)){
      if(DX_vector[j,] %in% colnames(H70_dataframe)){
        H70_dataframe_iter[j, which(colnames(H70_dataframe) == as.character(DX_vector[j, ]))] <- 1
      }
  }
  H70_dataframe <- H70_dataframe + H70_dataframe_iter
}
```

```{r}
unique_PR <- df %>%
  select(I10_PR1:I10_PR25) %>%
  lapply(unique)
combined_unique_PR <- unlist(unique_PR)
overall_unique_PR <- unique(combined_unique_PR)
overall_unique_PR
unique_PR <- overall_unique_PR[2:length(overall_unique_PR)]
unique_PR
```

```{r}
PR_data <- df[ , 2:26]
PR_dataframe <- matrix(0, nrow = nrow(df), ncol = length(unique_PR))
colnames(PR_dataframe) <- unique_PR
for(i in 1:25){
  PR_vector <- PR_data[ , i]
  PR_dataframe_iter <- matrix(0, nrow = nrow(df), ncol = length(unique_PR))
  for(j in 1:nrow(PR_vector)){
      if(PR_vector[j,] %in% colnames(PR_dataframe)){
        PR_dataframe_iter[j, which(colnames(PR_dataframe) == as.character(PR_vector[j, ]))] <- 1
      }
  }
  PR_dataframe <- PR_dataframe + PR_dataframe_iter
}
```

```{r}
total <- cbind(df, H70_dataframe)
dx_cols <- paste0("I10_DX", 1:40)
pr_cols <- paste0("I10_PR", 1:25)
total <- total %>% select(-all_of(c(dx_cols, pr_cols)))
total <- cbind(total, PR_dataframe)

total <- total %>%
  rename(
    B110F4 = `0B110F4`,
    B113F4 = `0B113F4`,
    B110Z4 = `0B110Z4`,
    CTS0ZZ = `0CTS0ZZ`,
    B114F4 = `0B114F4`,
    B113Z4 = `0B113Z4`,
    B110D6 = `0B110D6`
  )
```

##########################Modeling starts here#############################################

```{r}
library(caret)
library(ROSE)
library(randomForest)
library(pROC)
library(lme4)
library(vcd)
library(Hmisc)
```

```{r}
total <- total %>%
  mutate(across(6:ncol(total), factor))

total$AGE <- scale(total$AGE)
total$LOS <- scale(total$LOS)
total$readmission_mark <- as.factor(total$readmission_mark)
total$FEMALE <- as.factor(total$FEMALE)
```

```{r}
set.seed(123)
trainIndex <- createDataPartition(total$readmission_mark, p = 0.75, list = FALSE)
trainData <- total[trainIndex,]
testData <- total[-trainIndex,]
```

###################################################################################################

```{r}
# Applying SMOTE for imbalanced data handling
trainDataBalanced <- ovun.sample(readmission_mark ~ AGE+FEMALE+LOS+H7091+H7011+H70002+H70001+H7012+H7092+H70009+H70091+H7093+H70003+H70891+H70092+H7090+H7013+H70221+H70892+H70201+H7010+H70093+H70209+H70899+H70012+H70893+H70212+H70222+B110F4+B113F4+B110Z4+CTS0ZZ+B114F4+B113Z4+B110D6, data = trainData, method = "over", N = 20000)$data
```

```{r}
train_control <- trainControl(
  method = "cv",         # cross-validation
  number = 20,           # number of folds
  verboseIter = TRUE,    # print training iterations
  savePredictions = "final"  # save predictions for each fold
)

```

```{r}
# Define the tuning grid correctly with 'alpha' and 'lambda'
tune_grid <- expand.grid(
  alpha = c(0.0001, 0.001, 0.01, 0.1, 1),  # Elastic-net mixing parameter
  lambda = c(1e-4, 1e-3, 1e-2, 1e-1, 1)   # Regularization parameter
)

```

```{r}
set.seed(123)
model <- train(
  readmission_mark ~ AGE + FEMALE + LOS + H7091 + H7011 + H70002 + H70001 + H7012 + H7092 + H70009 + H70091 + H7093 + H70003 + H70891 + H70092 + H7090 + H7013 + H70221 + H70892 + H70201 + H7010 + H70093 + H70209 + H70899 + H70012 + H70893 + H70222 + B110F4 + B113F4 + B110Z4 + B114F4 + B113Z4 + B110D6 + CTS0ZZ,         # formula
  data = trainDataBalanced,       
  method = "glmnet",    
  trControl = train_control,
  tuneGrid = tune_grid,     # hyperparameter grid
  metric = "Accuracy"       # performance metric
)
```

```{r}
# Print the best model's details
print(model)

# Make predictions on new data
predictions <- predict(model, newdata = testData)

# Evaluate the model
confusionMatrix(predictions, testData$readmission)
```


##############################OLD VERSION###########################################################
```{r}
table(trainData$readmission_mark)
```


```{r}
# Checking the levels of each categorical variable
sapply(trainDataBalanced[, 1:ncol(trainDataBalanced)], function(x) {
  list(levels = levels(x), nlevels = length(levels(x)))
})

logiModel <- glm(readmission_mark ~ AGE + FEMALE + LOS + H7091 + H7011 + H70002 + H70001 + H7012 + H7092 + H70009 + H70091 + H7093 + H70003 + H70891 + H70092 + H7090 + H7013 + H70221 + H70892 + H70201 + H7010 + H70093 + H70209 + H70899 + H70012 + H70893 + H70222 + B110F4 + B113F4 + B110Z4 + B114F4 + B113Z4 + B110D6, data = trainDataBalanced, family = binomial())
```

```{r}
rfModel <- randomForest(readmission_mark ~ AGE + FEMALE + LOS + H7091 + H7011 + H70002 + H70001 + H7012 + H7092 + H70009 + H70091 + H7093 + H70003 + H70891 + H70092 + H7090 + H7013 + H70221 + H70892 + H70201 + H7010 + H70093 + H70209 + H70899 + H70012 + H70893 + H70212 + H70222 + B110F4 + B113F4 + B110Z4 + B114F4 + B113Z4 + B110D6, data = trainDataBalanced)
```

```{r}
evaluate_performance <- function(model, testData) {

  predProb <- predict(model, newdata = testData, type = "response")
  predClass <- ifelse(predProb > 0.5, "1", "0")
  
  rocCurve <- roc(testData$readmission, as.numeric(predProb))
  auc <- auc(rocCurve)
  
  cm <- confusionMatrix(as.factor(predClass), testData$readmission)
  
  list(auc = auc, confusionMatrix = cm)
}


logitEval <- evaluate_performance(logiModel, testData)
print(logitEval)
```

```{r}
predicted_probs <- predict(rfModel, newdata = testData, type = "response")
predicted_probs <- as.numeric(as.character(predicted_probs))

predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)  # Assuming binary classification
accuracy <- mean(predicted_classes == testData$readmission_mark)

print(accuracy)
```


#################################################Embedding###########################################################

```{r}
#library(reticulate)
#reticulate::install_python(version = '<version>')
py_run_string("print('Hello from Python')")
```

```{r}
py_config()
#py_install("tensorflow")

```
```{r}
X <- select(total, -readmission_mark)
y <- total$readmission_mark
```

```{r}
library(recipes)
library(rsample)

recipe_obj <- recipe(readmission_mark ~ AGE+FEMALE+H7091+H7011+H70002+H70001+H7012+H7092+H70009+H70091+H7093+H70003+H70891+H70092+H7090+H7013+H70221+H70892+H70201+H7010+H70093+H70209+H70899+H70012+H70893+H70212+H70222+B110F4 + B113F4 + B110Z4 + B114F4 + B113Z4 + B110D6, data = total) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  prep(training = total, retain = TRUE)
```

```{r}
X_encoded <- bake(recipe_obj, new_data = NULL)
```

```{r}
set.seed(123)  # for reproducibility
split <- initial_split(X_encoded, prop = 0.75)
train_data <- training(split)
test_data <- testing(split)
```

```{r}
library(keras3)
```

```{r}
train_features <- as.matrix(train_data %>% select(-readmission_mark))
train_target <- train_data$readmission_mark
```

```{r}
num_features <- ncol(train_data %>% select(-readmission_mark))
print(num_features)
```

```{r}
# Resetting the Keras session might help in some cases
k_clear_session()

model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(num_features)) %>%
  layer_dense(units = 1)  # output layer for regression

model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

# Convert training data to matrix format
train_features <- as.matrix(train_data %>% select(-readmission_mark))
train_target <- train_data$readmission_mark

# Fit the model
history <- model %>% fit(
  x = train_features, 
  y = train_target,
  epochs = 100,
  batch_size = 32,
  validation_split = 0.2
)
```


